{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3d7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "\n",
    "dict_list = []\n",
    "count = 0\n",
    "with open(\"data/review-New_York_10.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line_num, line in enumerate(f, start=1):\n",
    "        line = line.strip()  # remove leading/trailing whitespace\n",
    "        if not line:\n",
    "            continue  # skip empty lines\n",
    "        try:\n",
    "            # Try JSON parsing first (double quotes)\n",
    "            obj = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                # Fallback for Python dict strings (single quotes)\n",
    "                obj = ast.literal_eval(line)\n",
    "            except Exception:\n",
    "                print(f\"Skipping invalid line {line_num}: {line[:100]}...\")\n",
    "                continue\n",
    "        dict_list.append(obj)\n",
    "        count+=1\n",
    "        if count > 100000:\n",
    "            break\n",
    "\n",
    "print(f\"Total valid entries: {len(dict_list)}\")\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a2e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f677cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = df_reviews.dropna(subset=['text']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f498992",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03da6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "\n",
    "dict_list = []\n",
    "\n",
    "with open(\"data/meta-New_York.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line_num, line in enumerate(f, start=1):\n",
    "        line = line.strip()  # remove leading/trailing whitespace\n",
    "        if not line:\n",
    "            continue  # skip empty lines\n",
    "        try:\n",
    "            # Try JSON parsing first (double quotes)\n",
    "            obj = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                # Fallback for Python dict strings (single quotes)\n",
    "                obj = ast.literal_eval(line)\n",
    "            except Exception:\n",
    "                print(f\"Skipping invalid line {line_num}: {line[:100]}...\")\n",
    "                continue\n",
    "        dict_list.append(obj)\n",
    "\n",
    "print(f\"Total valid entries: {len(dict_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c1c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb2bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_reviews, df_meta, on='gmap_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db359080",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop_duplicates(subset=['name_x','text','gmap_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defbd73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289e5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_columns = ['text','resp','description','category','name_y', 'rating']\n",
    "final_df = merged_df[final_columns]\n",
    "\n",
    "for col in final_df.columns:\n",
    "    final_df[col] = final_df[col].apply(\n",
    "        lambda x: \", \".join(x) if isinstance(x, list) else x\n",
    "    )\n",
    "\n",
    "# Reset index so you get a new incrementing column\n",
    "final_df = final_df.reset_index(drop=True)\n",
    "final_df.index.name = \"id\"\n",
    "\n",
    "final_df = final_df.rename(columns={\n",
    "    \"name_y\": \"Business Name\",\n",
    "    \"category\": \"Category\",\n",
    "    \"description\": \"Description\",\n",
    "    \"text\": \"Review\",\n",
    "    \"rating\": \"Rating\",\n",
    "    \"resp\": \"Response\"\n",
    "})\n",
    "\n",
    "final_df[\"user_message\"] = (\n",
    "    \"Business Name: \" + final_df[\"Business Name\"].astype(str).fillna(\"N/A\") + \"\\n\"\n",
    "    \"Category: \" + final_df[\"Category\"].astype(str).fillna(\"N/A\") + \"\\n\"\n",
    "    \"Description: \" + final_df[\"Description\"].astype(str).fillna(\"N/A\") + \"\\n\"\n",
    "    \"Review: \" + final_df[\"Review\"].astype(str).fillna(\"N/A\") + \"\\n\"\n",
    "    \"Rating: \" + final_df[\"Rating\"].astype(str).fillna(\"N/A\") + \"\\n\"\n",
    "    \"Response: \" + final_df[\"Response\"].astype(str).fillna(\"N/A\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54651789",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"data.csv\", index=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c343cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37345d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['user_message'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c607d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(override=True)  # force overwrite existing environment variables\n",
    "\n",
    "load_dotenv()  # loads .env variables\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "print(\"API key loaded:\", api_key)  # just to check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14eb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97fd7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = '''\n",
    " <task>\n",
    "    Analyze Google reviews to gauge review quality by detecting problematic content.\n",
    "  </task>\n",
    "  \n",
    "  <detection_criteria>\n",
    "    <spam>Detect reviews that are fake, duplicate, or artificially generated</spam>\n",
    "    <advertisement>Detect reviews that primarily promote other businesses or services</advertisement>\n",
    "    <irrelevant_content>Detect reviews that don't relate to the actual business or location</irrelevant_content>\n",
    "    <non_visitor_rant>Detect rants from users who likely never visited the location</non_visitor_rant>\n",
    "    <toxicity>Detect reviews containing rudeness, racism, hate speech, harassment, or other toxic behavior</toxicity>\n",
    "  </detection_criteria>\n",
    "  \n",
    "  <labeling_instructions>\n",
    "    Label each detection category with 1 if the issue is present, 0 if not present.\n",
    "  </labeling_instructions>\n",
    "  \n",
    "  <input_format>\n",
    "    Review text will be provided for analysis.\n",
    "  </input_format>\n",
    "  \n",
    "  <response_format>\n",
    "    {\n",
    "      \"spam\": 0 or 1,\n",
    "      \"advertisement\": 0 or 1, \n",
    "      \"irrelevant_content\": 0 or 1,\n",
    "      \"non_visitor_rant\": 0 or 1,\n",
    "      \"toxicity\": 0 or 1\n",
    "    }\n",
    "  </response_format>\n",
    "  \n",
    "  <example>\n",
    "    Input: \"This place is terrible! I heard from my friend it's dirty and overpriced. Never going there!\"\n",
    "    Output: {\"spam\": 0, \"advertisement\": 0, \"irrelevant_content\": 0, \"non_visitor_rant\": 1, \"toxicity\": 0}\n",
    "  </example>\n",
    "  <example>\n",
    "    Input: \"Had dinner here last night with my family. The food was okay I guess, service was a bit slow. But speaking of great service, I have to mention that \n",
    "    I recently had my car detailed by Elite Auto Spa on Main Street and it was absolutely incredible! They made my 10-year-old Honda look brand new. \n",
    "    The owner Dave is super friendly and they use eco-friendly products. Only $89 for a full detail - such a steal! If anyone needs their car cleaned, \n",
    "    definitely check out Elite Auto Spa. They're open 7 days a week and take walk-ins. Tell them Mike sent you for 10% off your first visit!\n",
    "    Anyway, the restaurant was fine. Might come back sometime.\"\n",
    "    Output:  {\"spam\": 0, \"advertisement\": 1, \"irrelevant_content\": 1, \"non_visitor_rant\": 0, \"toxicity\": 0}\n",
    "  </example>\n",
    "  <example>\n",
    "    Input: \"This place is absolutely DISGUSTING and the staff are complete idiots who don't know anything about customer service!!! The manager Karen is a total b**** who needs to be fired immediately. Food gave me food poisoning and I'm going to sue them for everything they're worth.\n",
    "    EVERYONE NEEDS TO BOYCOTT THIS TERRIBLE RESTAURANT. They are probably money laundering and breaking health codes. I saw mexican ROACHES in the kitchen (I didn't but whatever).\n",
    "    By the way, if you want REAL Italian food, go to Tony's Pizza Palace downtown - they have the BEST pizza in the city and Maria the owner is amazing! Use code FOODIE20 for 20% off! They're having a grand opening special this week!\n",
    "    Also check out my food blog at totally-real-food-reviews.net where I expose restaurants like this one. Follow me on Instagram @foodcritic_real for more honest reviews!\n",
    "    DO NOT EAT HERE - YOU HAVE BEEN WARNED!!!\"\n",
    "    Output: {\"spam\": 0, \"advertisement\": 0, \"irrelevant_content\": 1, \"non_visitor_rant\": 0, \"toxicity\": 1}\n",
    "  </example>  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3285ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class MessageInfo(BaseModel):\n",
    "    spam: str\n",
    "    advertisement: str\n",
    "    irrelevant_content: str\n",
    "    non_visitor_rant: str\n",
    "    toxicity:str \n",
    "\n",
    "class Messages(BaseModel):\n",
    "    messages: List[MessageInfo]\n",
    "\n",
    "class BatchProcess():\n",
    "    def __init__(self, df:pd.DataFrame, start_index: int, batch_start_number:int, system_message:str, client:OpenAI):\n",
    "        \"\"\" \n",
    "        prepares raw jd from df, sends it to llms and writes outputs to json\n",
    "        \n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.start_index = start_index #index of the dataframe to START\n",
    "        self.batch_start_number = batch_start_number #batch number to START\n",
    "        self.system_message = system_message\n",
    "        self.client = client\n",
    "\n",
    "    def process(self, no_of_batches:int, batch_size:int) -> None:\n",
    "        #prepare the messages in the batch\n",
    "        for i in range(no_of_batches):\n",
    "            print(f\"Processing batch {i}...\")\n",
    "            \n",
    "            messages, row_ids, checkpoint = self.prepare_llm_inputs(self.df, self.start_index, batch_size) #process before llm\n",
    "            llm_structured_output = self.get_structured_output_from_llm(self.system_message, messages) #pass to llm, get responses\n",
    "\n",
    "            print(\"Processing row_ids...\", row_ids)\n",
    "\n",
    "            if len(llm_structured_output.messages) != len(row_ids):\n",
    "                print(messages)\n",
    "                print(llm_structured_output.messages)\n",
    "                raise ValueError(f\"Error: Expected batch size of {batch_size}, but got jobids: {len(row_ids)} and llmoutputs: {len(llm_structured_output.messages)}.\")\n",
    "\n",
    "            batch_jds = self.append_jb_ids(row_ids, llm_structured_output) #process output, append jd ids to the response\n",
    "\n",
    "            self.write_to_file(batch_jds, checkpoint, self.batch_start_number) #write to json for the batch\n",
    "            \n",
    "            self.start_index = checkpoint\n",
    "\n",
    "            self.batch_start_number += 1\n",
    "        \n",
    "\n",
    "\n",
    "       \n",
    "    # need to iterate through the rows in the df for that batch from the start checkpoint, \n",
    "    def prepare_llm_inputs(self, df: pd.DataFrame, checkpoint:int, batch_size:int) -> tuple[str, List[int], int]:\n",
    "        \"\"\" \n",
    "        This function iterates through the rows in the dataframe in that batch and prepares multiple raw JDs into one message/string for the llm\n",
    "        returns the updated checkpoint, input prompt which is a string and the list of their job ids for addition later\n",
    "        \"\"\"\n",
    "        messages =\"\"\n",
    "        job_ids = []\n",
    "        counter = 1\n",
    "        for i in range(checkpoint, checkpoint+batch_size):\n",
    "            job_id = int(df.iloc[i]['id'])\n",
    "            raw_jd = df.iloc[i]['user_message'].strip()\n",
    "            job_ids.append(job_id)\n",
    "            messages += f\"<Job {counter}>\\n{raw_jd}\\n</Job {counter}>\\n\\n\"\n",
    "            counter+=1\n",
    "        # Check if the number of job ids does not match the batch size\n",
    "        if len(job_ids) != batch_size:\n",
    "            \n",
    "            raise ValueError(f\"Error: Expected batch size of {batch_size}, but got {len(job_ids)}.\")\n",
    "\n",
    "        return (messages, job_ids, checkpoint + batch_size)\n",
    "    \n",
    "\n",
    "            \n",
    "    def get_structured_output_from_llm(self, system_message: str, user_message:str) -> Messages:\n",
    "        \"\"\" \n",
    "        sends the messages to gemini, returns a Messages object\n",
    "        \"\"\"\n",
    "        response = self.client.beta.chat.completions.parse(\n",
    "            model=\"gemini-2.0-flash-lite\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_message\n",
    "                }\n",
    "            ],\n",
    "            response_format=Messages,\n",
    "        )\n",
    "        return response.choices[0].message.parsed\n",
    "    \n",
    "    # we need to append the job ids to each message, saves output to json file\n",
    "    def append_jb_ids(self, job_ids:List, messages_list: Messages) -> List:\n",
    "        batch_jds = []\n",
    "        for idx, message in enumerate(messages_list.messages):\n",
    "            temp = message.model_dump()\n",
    "            #print(temp, idx)\n",
    "            temp['job_id'] = job_ids[idx]\n",
    "            batch_jds.append(temp)\n",
    "\n",
    "        return batch_jds\n",
    "    \n",
    "    def write_to_file(self, batch_jds:List, row_checkpoint, batch_no) -> None:\n",
    "            row_checkpoint -= 1\n",
    "            dir_name = \"extracted_labels\"\n",
    "            os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "            filename = f\"{dir_name}/batch_{batch_no}_row_{row_checkpoint}_extracted_labels.json\"\n",
    "\n",
    "            with open(filename, \"w\") as f:\n",
    "                json.dump(batch_jds, f, indent=4) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b3def",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83071fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = BatchProcess(shuffled_df, 9700, 485, system_message, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c557ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.process(500, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b14d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages, job_ids, checkpoint = batch.prepare_llm_inputs(fake_df, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4928dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = batch.get_structured_output_from_llm(system_message, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62382ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880846d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(10).iloc[9]['user_message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cf5e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# get all json files in the folder\n",
    "json_files = glob.glob(\"extracted_labels/*.json\")\n",
    "\n",
    "combined_data = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            combined_data.extend(data)   # merge lists\n",
    "        else:\n",
    "            combined_data.append(data)   # append objects\n",
    "\n",
    "# save into one file\n",
    "with open(\"combined.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(combined_data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe19dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f72f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data_df = pd.DataFrame(combined_data)\n",
    "combined_data_df = combined_data_df.rename(columns={\"job_id\": \"id\"})\n",
    "labeled_final = pd.merge(combined_data_df, shuffled_df, on=\"id\", how=\"left\")\n",
    "\n",
    "labeled_final.to_csv(\"final_labeled.csv\", index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a9f34e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1998bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
